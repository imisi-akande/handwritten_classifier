{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] Save Path\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/AkandeImisioluwa/anaconda3/envs/news/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages')\n",
    "import numpy as np\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import backward_layer as backward\n",
    "from tqdm import tqdm\n",
    "from optimizer import *\n",
    "from forward_layer import ForwardLayers\n",
    "from backward_layer import BackwardLayers\n",
    "from utility import ModelHelpers\n",
    "\n",
    "\n",
    "class Model:\n",
    "    \"\"\"\n",
    "    With the aim of abstraction, this class entails the model entity. The model\n",
    "    defines a method called 'convolution_layers' that integrates the forward and\n",
    "    the backward operations of the convolutional neural network. It takes the \n",
    "    network’s parameters and hyperparameters as inputs and returns the gradients\n",
    "    as output.\n",
    "    \"\"\"\n",
    "    \n",
    "    def convolution_layers(self,image=None, label=None, parameters=None, \n",
    "                            conv_s=None, pool_filter=None, pool_stride=None):\n",
    "        \"\"\"\n",
    "        A fully connected network that uses the Softmax and creates the \n",
    "        probabilities. Given the number of classes (10 in total) \n",
    "        and the size of each training image example (28x28px.), this network \n",
    "        architecture implements the task of digit recognition. \n",
    "        The network uses convolutional layers followed by a \n",
    "        max pooling operation to extract features from the input image. \n",
    "        After the max pooling operation, the representation was flattened and \n",
    "        passed through a Multi-Layer Perceptron (MLP) to carry out the task of \n",
    "        classification.\n",
    "        Parameters\n",
    "        ----------\n",
    "        image : None\n",
    "            The input image dimension\n",
    "        label : None\n",
    "            The predicted output (0 to 9)\n",
    "        parameters : None\n",
    "            The filters, weights and bias utilized\n",
    "        conv_s : None\n",
    "            The number of strides used in convolution layers        \n",
    "        pool_filt : None\n",
    "            The dimension of filters(kernels) (height, width)\n",
    "        pool_stride : None  \n",
    "            The number of strides used on the pooling layer  \n",
    "        Returns\n",
    "        -------\n",
    "        gradients and loss: The result measures how the cost changes in the \n",
    "            vicinity of the current position respect to the inputs parameters\n",
    "            and hyperparameters.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        [f1, f2, w3, w4, b1, b2, b3, b4] = parameters \n",
    "    \n",
    "        ################################################\n",
    "        ############## Forward Operation ###############\n",
    "        ################################################\n",
    "        forward = ForwardLayers(image)\n",
    "        # first convolution operation\n",
    "        conv1 = forward.forward_convolution_layer(f1, b1, conv_s)\n",
    "        conv1[conv1<=0] = 0 # pass through ReLU non-linearity\n",
    "        forward = ForwardLayers(conv1)\n",
    "        # second convolution operation\n",
    "        conv2 = forward.forward_convolution_layer(f2, b2, conv_s) \n",
    "        conv2[conv2<=0] = 0 # pass through ReLU non-linearity\n",
    "        forward = ForwardLayers(conv2)\n",
    "        # maxpooling operation\n",
    "        pooled = forward.forward_max_pool(pool_filter, pool_stride) \n",
    "    \n",
    "        (nf2, dim2, _) = pooled.shape\n",
    "        fc = pooled.reshape((nf2 * dim2 * dim2, 1)) # flatten pooled layer\n",
    "    \n",
    "        z = w3.dot(fc) + b3 # first dense layer\n",
    "        z[z<=0] = 0 # pass through ReLU non-linearity\n",
    "    \n",
    "        out = w4.dot(z) + b4 # second dense layer\n",
    "\n",
    "        # predict class probabilities with the softmax activation function\n",
    "        probs = forward.softmax(out) \n",
    "    \n",
    "        loss = forward.categorical_cross_entropy(probs, label) # categorical cross-entropy loss\n",
    "        \n",
    "        ################################################\n",
    "        ############# Backward Operation ###############\n",
    "        ################################################\n",
    "        dout = probs - label # derivative of loss w.r.t. final dense layer output\n",
    "        dw4 = dout.dot(z.T) # loss gradient of final dense layer weights\n",
    "        # loss gradient of final dense layer biases\n",
    "        db4 = np.sum(dout, axis = 1).reshape(b4.shape) \n",
    "    \n",
    "        dz = w4.T.dot(dout) # loss gradient of first dense layer outputs \n",
    "        dz[z<=0] = 0 # backpropagate through ReLU \n",
    "        dw3 = dz.dot(fc.T)\n",
    "        db3 = np.sum(dz, axis = 1).reshape(b3.shape)\n",
    "\n",
    "        # loss gradients of fully-connected layer (pooling layer)\n",
    "        dfc = w3.T.dot(dz) \n",
    "        # reshape fully connected into dimensions of pooling layer\n",
    "        dpool = dfc.reshape(pooled.shape) \n",
    "        backward =  BackwardLayers(pool_stride)\n",
    "        # backprop through the max-pooling layer(only neurons with highest\n",
    "        # activation in window get updated)\n",
    "        dconv2 = backward.maxpoolBackward(dpool, conv2, pool_filter) \n",
    "        dconv2[conv2<=0] = 0 # backpropagate through ReLU\n",
    "        backward = BackwardLayers(conv_s)\n",
    "        # backpropagate previous gradient through second convolutional layer.\n",
    "        dconv1, df2, db2 = backward.convolutionBackward(dconv2, conv1, f2) \n",
    "        dconv1[conv1<=0] = 0 # backpropagate through ReLU\n",
    "        backward = BackwardLayers(conv_s)\n",
    "        # backpropagate previous gradient through first convolutional layer.\n",
    "        dimage, df1, db1 = backward.convolutionBackward(dconv1, image, f1) \n",
    "    \n",
    "        grads = [df1, df2, dw3, dw4, db1, db2, db3, db4] \n",
    "    \n",
    "        return grads, loss\n",
    "\n",
    "    \n",
    "    #####################################################\n",
    "    ##################### Training Ops###################\n",
    "    #####################################################\n",
    "\n",
    "    def train(num_classes = 10, lr = 0.0001, bheta1 = 0.95, bheta2 = 0.99, \n",
    "                img_dim = 28, img_depth = 1, f_layer1 = 10, f_layer2 = 5, num_filt1 = 32,\n",
    "                num_filt2 = 16, batch_size = 100, num_epochs = 200, save_path = 'parameters.pkl'):\n",
    "        \"\"\"\n",
    "        Training method is an approach to ensure the model is learning on a \n",
    "        particular set of data. In this case the model is trained on MNIST \n",
    "        dataset so that the Machine can learn and generally predict. To illustrate,\n",
    "        the machine can predict that an handwritten digit is '3' out of the \n",
    "        remaining classes(0,1,2,4,5,6,7,8,9).\n",
    "        Given 60,000 training dataset, the model is built. This model generally\n",
    "        will try to predict one variable based on all the others as described \n",
    "        above. \n",
    "        This training method implements Adam optimization algorithm for optimization.\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_classes: 10(int)\n",
    "            The output labels (0 - 9)\n",
    "        lr : 0.0001(float)\n",
    "            Learning rate is a hyper-parameter that controls how much the \n",
    "            weights of the network is being adjusted with respect to the \n",
    "            loss gradient. The lower the value, the slower we travel along the \n",
    "            downward slope. A low learning rate was utilized(using a low learning rate) \n",
    "            in order to ensure that no local minima was missed.\n",
    "        bheta1: 0.95(float)\n",
    "            The exponential decay rate for the first moment estimates \n",
    "        bheta2: 0.99(float)\n",
    "            The exponential decay rate for the second-moment estimates \n",
    "        img_dim : 28 * 28\n",
    "            The dimension of the image (h * w)         \n",
    "        image_depth : 1\n",
    "            The channel of the image G (greyscale). If RGB then image depth = 3\n",
    "        f_layer1 : 10\n",
    "            The filter dimensions of the first convolution layer   \n",
    "        f_layer2 : 5\n",
    "            The filter dimensions of the second convolution layer   \n",
    "        num_filt1 : 32\n",
    "            Number of output channels of the first convolution layer\n",
    "        num_filt2 : 16\n",
    "            Number of output channels of the second convolution layer\n",
    "        batch_size : 100\n",
    "            The total number of training examples present in a batch\n",
    "        num_epochs : 200 \n",
    "            The number of times the entire dataset in the batch passed \n",
    "            forward and backward through the network\n",
    "        save_path : parameters.pkl\n",
    "            The network hyperparameters saved file.               \n",
    "        Returns\n",
    "        -------\n",
    "        cost : computes the average of the loss functions of the entire \n",
    "            training sets\n",
    "        \"\"\"\n",
    "        # training data\n",
    "        m =60000\n",
    "        util = ModelHelpers()\n",
    "        X = util.extract_data('mnist_data/train-images-idx3-ubyte.gz', m, img_dim)\n",
    "        x_shaped = np.reshape(X, [-1, 28, 28, 1])\n",
    "        y_dash = util.extract_labels('mnist_data/train-labels-idx1-ubyte.gz', m).reshape(m,1)\n",
    "        X-= int(np.mean(x_shaped))\n",
    "        X/= int(np.std(X))\n",
    "        train_data = np.hstack((X,y_dash))\n",
    "    \n",
    "        np.random.shuffle(train_data)\n",
    "\n",
    "        ## Initializing all the parameters\n",
    "        f1, f2, w3, w4 = (num_filt1 ,img_depth,f_layer1,f_layer1), (num_filt2 ,num_filt1,f_layer2,f_layer2), (1024,784), (10, 1024)\n",
    "\n",
    "        f1 = util.initializeFilter(f1)\n",
    "        f2 = util.initializeFilter(f2)\n",
    "        w3 = util.initializeWeight(w3)\n",
    "        w4 = util.initializeWeight(w4)\n",
    "\n",
    "        b1 = np.zeros((f1.shape[0],1))\n",
    "        b2 = np.zeros((f2.shape[0],1))\n",
    "        b3 = np.zeros((w3.shape[0],1))\n",
    "        b4 = np.zeros((w4.shape[0],1))\n",
    "\n",
    "        parameters = [f1, f2, w3, w4, b1, b2, b3, b4]\n",
    "\n",
    "        cost = []\n",
    "\n",
    "        print(\"Learning-rate:\"+str(lr)+\", Batch Size:\"+str(batch_size))\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            np.random.shuffle(train_data)\n",
    "            batches = [train_data[k:k + batch_size] for k in range(0, train_data.shape[0], batch_size)]\n",
    "\n",
    "            t = tqdm(batches)\n",
    "            for x,batch in enumerate(t):\n",
    "                parameters, cost = adamsGradientDescent(batch, num_classes, lr, img_dim, img_depth, bheta1, bheta2, parameters, cost)\n",
    "                t.set_description(\"Cost: %.2f\" % (cost[-1]))\n",
    "            \n",
    "        to_save = [parameters, cost]\n",
    "    \n",
    "        with open(save_path, 'wb') as file:\n",
    "            pickle.dump(to_save, file)\n",
    "        \n",
    "        return cost\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='A module for training a convolutional neural network.')\n",
    "    parser.add_argument('save_path', metavar = 'Save Path', help='File that stores parameters.')\n",
    "\n",
    "    #####################################################\n",
    "    ##################### Measure Performance############\n",
    "    #####################################################\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "    \n",
    "        args = parser.parse_args()\n",
    "        save_path = args.save_path\n",
    "        model = Model()\n",
    "        cost = train(save_path = save_path)\n",
    "\n",
    "        parameters, cost = pickle.load(open(save_path, 'rb'))\n",
    "        [f1, f2, w3, w4, b1, b2, b3, b4] = parameters\n",
    "    \n",
    "        # Plot cost over number of iterations\n",
    "        plt.plot(cost, 'r')\n",
    "        plt.xlabel('Number of Iterations')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.legend('Loss', loc='upper right')\n",
    "        plt.show()\n",
    "\n",
    "        # Get test data\n",
    "        m =10000\n",
    "        X = util.extract_data('./mnist_data/t10k-images-idx3-ubyte.gz', m, 28)\n",
    "        y_dash = util.extract_labels('./mnist_data/t10k-labels-idx1-ubyte.gz', m).reshape(m,1)\n",
    "        \n",
    "        # Normalize the data\n",
    "        X-= int(np.mean(X)) # subtract mean\n",
    "        X/= int(np.std(X)) # divide by standard deviation\n",
    "        test_data = np.hstack((X,y_dash))\n",
    "    \n",
    "        X = test_data[:,0:-1]\n",
    "        X = X.reshape(len(test_data), 1, 28, 28)\n",
    "        y = test_data[:,-1]\n",
    "\n",
    "        corr = 0\n",
    "        digit_count = [0 for i in range(10)]\n",
    "        digit_correct = [0 for i in range(10)]\n",
    "   \n",
    "        print()\n",
    "        print(\"Next, Computing accuracy operation on the test dataset:\")\n",
    "\n",
    "        t = tqdm(range(len(X)), leave=True)\n",
    "\n",
    "        for i in t:\n",
    "            x = X[i]\n",
    "            pred, prob = predict(x, f1, f2, w3, w4, b1, b2, b3, b4)\n",
    "            digit_count[int(y[i])]+=1\n",
    "            if pred==y[i]:\n",
    "                corr+=1\n",
    "                digit_correct[pred]+=1\n",
    "\n",
    "            t.set_description(\"Acc:%0.2f%%\" % (float(corr/(i+1))*100))\n",
    "        \n",
    "        print(\"Overall Accuracy: %.2f\" % (float(corr/len(test_data)*100)))\n",
    "        x = np.arange(10)\n",
    "        digit_recall = [x/y for x,y in zip(digit_correct, digit_count)]\n",
    "        plt.xlabel('Digits')\n",
    "        plt.ylabel('Recall')\n",
    "        plt.title(\"Recall on Test Set\")\n",
    "        plt.bar(x,digit_recall)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting mnist_data/train-labels-idx1-ubyte.gz\n",
      "Learning-rate:0.0001, Batch Size:100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cost: 0.02: 100%|██████████| 600/600 [08:58<00:00,  1.12it/s]\n",
      "Cost: 0.02: 100%|██████████| 600/600 [06:09<00:00,  1.62it/s]\n",
      "Cost: 0.02: 100%|██████████| 600/600 [05:35<00:00,  1.79it/s]\n",
      "Cost: 0.01: 100%|██████████| 600/600 [08:47<00:00,  1.14it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [08:10<00:00,  1.22it/s]\n",
      "Cost: 0.01: 100%|██████████| 600/600 [08:03<00:00,  1.24it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [06:23<00:00,  1.57it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [06:11<00:00,  1.61it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [07:40<00:00,  1.30it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [07:41<00:00,  1.30it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [07:22<00:00,  1.36it/s]\n",
      "Cost: 0.01: 100%|██████████| 600/600 [03:56<00:00,  2.54it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [03:56<00:00,  2.53it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [03:52<00:00,  2.58it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [03:52<00:00,  2.58it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [05:34<00:00,  1.80it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:58<00:00,  2.01it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [03:52<00:00,  2.58it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [03:52<00:00,  2.58it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [03:53<00:00,  2.57it/s]\n",
      "Cost: 0.01: 100%|██████████| 600/600 [04:24<00:00,  2.27it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [05:43<00:00,  1.75it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:18<00:00,  2.32it/s]\n",
      "Cost: 0.01: 100%|██████████| 600/600 [03:59<00:00,  2.50it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:02<00:00,  2.47it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:00<00:00,  2.49it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:03<00:00,  2.47it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:12<00:00,  2.38it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:00<00:00,  2.49it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:01<00:00,  2.48it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:00<00:00,  2.50it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:01<00:00,  2.48it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:00<00:00,  2.50it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:01<00:00,  2.48it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:01<00:00,  2.49it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:01<00:00,  2.49it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:01<00:00,  2.48it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:01<00:00,  2.48it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:02<00:00,  2.48it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:01<00:00,  2.49it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:01<00:00,  2.49it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:03<00:00,  2.47it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:13<00:00,  2.37it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:05<00:00,  2.45it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:00<00:00,  2.49it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:01<00:00,  2.48it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:00<00:00,  2.50it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:00<00:00,  2.49it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:02<00:00,  2.48it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:01<00:00,  2.49it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:02<00:00,  2.47it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:01<00:00,  2.49it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:01<00:00,  2.48it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:01<00:00,  2.48it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:01<00:00,  2.49it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:02<00:00,  2.47it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:04<00:00,  2.46it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:09<00:00,  2.40it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:25<00:00,  2.26it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:03<00:00,  2.47it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:03<00:00,  2.46it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:02<00:00,  2.48it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:03<00:00,  2.47it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:02<00:00,  2.47it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:02<00:00,  2.47it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:02<00:00,  2.47it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:02<00:00,  2.48it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:03<00:00,  2.46it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:01<00:00,  2.48it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:02<00:00,  2.47it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:02<00:00,  2.48it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:02<00:00,  2.48it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:03<00:00,  2.47it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:02<00:00,  2.47it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:17<00:00,  2.33it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:01<00:00,  2.48it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:01<00:00,  2.48it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:01<00:00,  2.48it/s]\n",
      "Cost: 0.05: 100%|██████████| 600/600 [04:01<00:00,  2.48it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:04<00:00,  2.46it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:01<00:00,  2.49it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:03<00:00,  2.46it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:02<00:00,  2.48it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:03<00:00,  2.46it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:03<00:00,  2.46it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:02<00:00,  2.47it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:03<00:00,  2.47it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:02<00:00,  2.47it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:02<00:00,  2.47it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:10<00:00,  2.40it/s]\n",
      "Cost: 0.01: 100%|██████████| 600/600 [04:01<00:00,  2.49it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:03<00:00,  2.46it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:02<00:00,  2.48it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:03<00:00,  2.47it/s]\n",
      "Cost: 0.02: 100%|██████████| 600/600 [04:02<00:00,  2.47it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:03<00:00,  2.47it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:03<00:00,  2.46it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:02<00:00,  2.47it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:03<00:00,  2.46it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:03<00:00,  2.46it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:04<00:00,  2.46it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:04<00:00,  2.46it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:02<00:00,  2.47it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:15<00:00,  2.35it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:28<00:00,  2.23it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:43<00:00,  2.12it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:15<00:00,  2.35it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:16<00:00,  2.34it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:08<00:00,  2.42it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:11<00:00,  2.39it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:26<00:00,  2.25it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [06:20<00:00,  1.58it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [05:21<00:00,  1.87it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [05:08<00:00,  1.95it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:36<00:00,  2.17it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:38<00:00,  2.16it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [06:25<00:00,  1.56it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [06:22<00:00,  1.57it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:38<00:00,  2.15it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:43<00:00,  2.11it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [05:34<00:00,  1.79it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [06:24<00:00,  1.56it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [07:07<00:00,  1.40it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [04:43<00:00,  2.12it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [05:25<00:00,  1.84it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [07:40<00:00,  1.30it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [07:04<00:00,  1.41it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [11:29<00:00,  1.15s/it]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [14:30<00:00,  1.45s/it]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [09:09<00:00,  1.09it/s]\n",
      "Cost: 0.01: 100%|██████████| 600/600 [11:21<00:00,  1.14s/it]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [09:19<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cost: 0.00: 100%|██████████| 600/600 [13:50<00:00,  1.38s/it]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [08:59<00:00,  1.11it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [06:15<00:00,  1.60it/s]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [46:58<00:00,  4.70s/it]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [5:57:31<00:00, 35.75s/it]  \n",
      "Cost: 0.00: 100%|██████████| 600/600 [11:03<00:00,  1.11s/it]\n",
      "Cost: 0.00: 100%|██████████| 600/600 [08:56<00:00,  1.12it/s]\n",
      "Cost: 0.00:  95%|█████████▍| 569/600 [06:24<00:20,  1.48it/s]"
     ]
    }
   ],
   "source": [
    "%run network.py parameters.pkl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
