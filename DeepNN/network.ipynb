{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages')\n",
    "import numpy as np\n",
    "import argparse\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import backward_layer as backward\n",
    "from tqdm import tqdm\n",
    "from optimizer import *\n",
    "from forward_layer import ForwardLayers\n",
    "from backward_layer import BackwardLayers\n",
    "from utility import ModelHelpers\n",
    "\n",
    "\n",
    "class Model:\n",
    "    \"\"\"\n",
    "    With the aim of abstraction, this class entails the model entity. The model\n",
    "    defines a method called 'convolution_layers' that integrates the forward and\n",
    "    the backward operations of the convolutional neural network. It takes the \n",
    "    networkâ€™s parameters and hyperparameters as inputs and returns the gradients\n",
    "    as output.\n",
    "    \"\"\"\n",
    "    \n",
    "    def convolution_layers(self,image=None, actual_label=None, parameters=None, \n",
    "                            conv_stride=None, pool_filter=None, pool_stride=None):\n",
    "        \"\"\"\n",
    "        A fully connected network that uses the Softmax and creates the \n",
    "        probabilities. Given the number of classes (10 in total) \n",
    "        and the size of each training image example (28x28px.), this network \n",
    "        architecture implements the task of digit recognition. \n",
    "        The network uses convolutional layers followed by a \n",
    "        max pooling operation to extract features from the input image. \n",
    "        After the max pooling operation, the representation was flattened and \n",
    "        passed through a Multi-Layer Perceptron (MLP) to carry out the task of \n",
    "        classification.\n",
    "        Parameters\n",
    "        ----------\n",
    "        image : None\n",
    "            The input image dimension\n",
    "        actual_label : None\n",
    "            The predicted output (0 to 9)\n",
    "        parameters : None\n",
    "            The filters, weights and bias utilized\n",
    "        conv_stride : None\n",
    "            The number of strides used in convolution layers        \n",
    "        pool_filt : None\n",
    "            The dimension of filters(kernels) (height, width)\n",
    "        pool_stride : None  \n",
    "            The number of strides used on the pooling layer  \n",
    "        Returns\n",
    "        -------\n",
    "        gradients and loss: The result measures how the cost changes in the \n",
    "            vicinity of the current position respect to the inputs parameters\n",
    "            and hyperparameters.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        [filter_one, filter_two, weight_three, weight_four, bias_one, bias_two, bias_three, bias_four] = parameters \n",
    "    \n",
    "        ################################################\n",
    "        ############## Forward Operation ###############\n",
    "        ################################################\n",
    "        forward = ForwardLayers(image)\n",
    "        # first convolution operation\n",
    "        conv1 = forward.forward_convolution_layer(filter_one, bias_one, conv_stride)\n",
    "        conv1[conv1<=0] = 0 # pass through ReLU non-linearity\n",
    "        forward = ForwardLayers(conv1)\n",
    "        # second convolution operation\n",
    "        conv2 = forward.forward_convolution_layer(filter_two, bias_two, conv_stride) \n",
    "        conv2[conv2<=0] = 0 # pass through ReLU non-linearity\n",
    "        forward = ForwardLayers(conv2)\n",
    "        # maxpooling operation\n",
    "        pooled = forward.forward_maxpool_layer(pool_filter, pool_stride) \n",
    "    \n",
    "        (nf2, dim2, _) = pooled.shape\n",
    "        fc = pooled.reshape((nf2 * dim2 * dim2, 1)) # flatten pooled layer\n",
    "    \n",
    "        z = weight_three.dot(fc) + bias_three # first dense layer\n",
    "        z[z<=0] = 0 # pass through ReLU non-linearity\n",
    "    \n",
    "        output = weight_four.dot(z) + bias_four # second dense layer\n",
    "\n",
    "        # predict class probabilities with the softmax activation function\n",
    "        desired_label = forward.softmax(output) \n",
    "    \n",
    "        loss = forward.categorical_cross_entropy(desired_label, actual_label) # categorical cross-entropy loss\n",
    "        \n",
    "        ################################################\n",
    "        ############# Backward Operation ###############\n",
    "        ################################################\n",
    "        doutput = desired_label - actual_label # derivative of loss w.r.t. final dense layer output\n",
    "        dweight_four = doutput.dot(z.T) # loss gradient of final dense layer weights\n",
    "        # loss gradient of final dense layer biases\n",
    "        dbias_four = np.sum(doutput, axis = 1).reshape(bias_four.shape) \n",
    "    \n",
    "        dz = weight_four.T.dot(doutput) # loss gradient of first dense layer outputs \n",
    "        dz[z<=0] = 0 # backpropagate through ReLU \n",
    "        dweight_three  = dz.dot(fc.T)\n",
    "        dbias_three = np.sum(dz, axis = 1).reshape(bias_three.shape)\n",
    "\n",
    "        # loss gradients of fully-connected layer (pooling layer)\n",
    "        dfc = weight_three.T.dot(dz) \n",
    "        # reshape fully connected into dimensions of pooling layer\n",
    "        dmax_pool = dfc.reshape(pooled.shape) \n",
    "        backward =  BackwardLayers(pool_stride)\n",
    "        # backprop through the max-pooling layer(only neurons with highest\n",
    "        # activation in window get updated)\n",
    "        dconv2 = backward.maxpool_backward(dmax_pool, conv2, pool_filter) \n",
    "        dconv2[conv2<=0] = 0 # backpropagate through ReLU\n",
    "        backward = BackwardLayers(conv_stride)\n",
    "        # backpropagate previous gradient through second convolutional layer.\n",
    "        dconv1, dfilter_two, dbias_two = backward.convolution_backward(dconv2, conv1, filter_two) \n",
    "        dconv1[conv1<=0] = 0 # backpropagate through ReLU\n",
    "        backward = BackwardLayers(conv_stride)\n",
    "        # backpropagate previous gradient through first convolutional layer.\n",
    "        dimage, dfilter_one, dbias_one = backward.convolution_backward(dconv1, image, filter_one) \n",
    "    \n",
    "        gradients = [dfilter_one, dfilter_two, dweight_three, dweight_four, dbias_one, dbias_two, dbias_three, dbias_four] \n",
    "    \n",
    "        return gradients, loss\n",
    "\n",
    "    \n",
    "    #####################################################\n",
    "    ##################### Training Ops###################\n",
    "    #####################################################\n",
    "\n",
    "    def train(num_classes = 10, learning_rate = 0.0001, bheta1 = 0.95, bheta2 = 0.99, \n",
    "                img_dim = 28, img_depth = 1, f_layer1 = 10, f_layer2 = 5, num_filt1 = 32,\n",
    "                num_filt2 = 16, batch_size = 100, num_epochs = 1, save_path = 'parameters.pkl'):\n",
    "        \"\"\"\n",
    "        Training method is an approach to ensure the model is learning on a \n",
    "        particular set of data. In this case the model is trained on MNIST \n",
    "        dataset so that the Machine can learn and generally predict. To illustrate,\n",
    "        the machine can predict that an handwritten digit is '3' out of the \n",
    "        remaining classes(0,1,2,4,5,6,7,8,9).\n",
    "        Given 60,000 training dataset, the model is built. This model generally\n",
    "        will try to predict one variable based on all the others as described \n",
    "        above. \n",
    "        This training method implements Adam optimization algorithm for optimization.\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_classes: 10(int)\n",
    "            The output labels (0 - 9)\n",
    "        learning_rate : 0.0001(float)\n",
    "            Learning rate is a hyper-parameter that controls how much the \n",
    "            weights of the network is being adjusted with respect to the \n",
    "            loss gradient. The lower the value, the slower we travel along the \n",
    "            downward slope. A low learning rate was utilized(using a low learning rate) \n",
    "            in order to ensure that no local minima was missed.\n",
    "        bheta1: 0.95(float)\n",
    "            The exponential decay rate for the first moment estimates \n",
    "        bheta2: 0.99(float)\n",
    "            The exponential decay rate for the second-moment estimates \n",
    "        img_dim : 28 * 28\n",
    "            The dimension of the image (height * width)         \n",
    "        image_depth : 1\n",
    "            The channel of the image G (greyscale). If RGB then image depth = 3\n",
    "        f_layer1 : 10\n",
    "            The filter dimensions of the first convolution layer   \n",
    "        f_layer2 : 5\n",
    "            The filter dimensions of the second convolution layer   \n",
    "        num_filt1 : 32\n",
    "            Number of output channels of the first convolution layer\n",
    "        num_filt2 : 16\n",
    "            Number of output channels of the second convolution layer\n",
    "        batch_size : 100\n",
    "            The total number of training examples present in a batch\n",
    "        num_epochs : 200 \n",
    "            The number of times the entire dataset in the batch passed \n",
    "            forward and backward through the network\n",
    "        save_path : parameters.pkl\n",
    "            The network hyperparameters saved file.               \n",
    "        Returns\n",
    "        -------\n",
    "        cost : computes the average of the loss functions of the entire \n",
    "            training sets\n",
    "        \"\"\"\n",
    "        # training data\n",
    "        m = 60000\n",
    "        util = ModelHelpers()\n",
    "        X = util.extract_dataset('mnist_data/train-images-idx3-ubyte.gz', m, img_dim)\n",
    "        x_shaped = np.reshape(X, [-1, 28, 28, 1])\n",
    "        y_dash = util.extract_desired_labels('mnist_data/train-labels-idx1-ubyte.gz', m).reshape(m,1)\n",
    "        X-= int(np.mean(x_shaped))\n",
    "        X/= int(np.std(X))\n",
    "        train_data = np.hstack((X,y_dash))\n",
    "    \n",
    "        np.random.shuffle(train_data)\n",
    "\n",
    "        ## Initializing all the parameters\n",
    "        filter_one, filter_two, weight_three , weight_four = (num_filt1 ,img_depth,f_layer1,f_layer1), (num_filt2 ,num_filt1,f_layer2,f_layer2), (1024,784), (10, 1024)\n",
    "\n",
    "        filter_one = util.initialize_filter(filter_one)\n",
    "        filter_two = util.initialize_filter(filter_two)\n",
    "        weight_three = util.initialize_weight(weight_three)\n",
    "        weight_four = util.initialize_weight(weight_four)\n",
    "\n",
    "        bias_one = np.zeros((filter_one.shape[0],1))\n",
    "        bias_two = np.zeros((filter_two.shape[0],1))\n",
    "        bias_three = np.zeros((weight_three.shape[0],1))\n",
    "        bias_four = np.zeros((weight_four.shape[0],1))\n",
    "\n",
    "        parameters = [filter_one, filter_two, weight_three, weight_four, bias_one, bias_two, bias_three, bias_four]\n",
    "\n",
    "        cost = []\n",
    "\n",
    "        print(\"Learning-rate:\"+str(learning_rate)+\", Batch Size:\"+str(batch_size))\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            np.random.shuffle(train_data)\n",
    "            batches = [train_data[k:k + batch_size] for k in range(0, train_data.shape[0], batch_size)]\n",
    "\n",
    "            t = tqdm(batches)\n",
    "            for x,batch in enumerate(t):\n",
    "                parameters, cost = adamsGradientDescent(batch, num_classes, learning_rate, img_dim, img_depth, bheta1, bheta2, parameters, cost)\n",
    "                t.set_description(\"Cost: %.2f\" % (cost[-1]))\n",
    "            \n",
    "        to_save = [parameters, cost]\n",
    "    \n",
    "        with open(save_path, 'wb') as file:\n",
    "            pickle.dump(to_save, file)\n",
    "        \n",
    "        return cost\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='A module for training a convolutional neural network.')\n",
    "    parser.add_argument('save_path', metavar = 'Save Path', help='File that stores parameters.')\n",
    "\n",
    "    #####################################################\n",
    "    ##################### Measure Performance############\n",
    "    #####################################################\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "    \n",
    "        args = parser.parse_args()\n",
    "        save_path = args.save_path\n",
    "        model = Model()\n",
    "        cost = train(save_path = save_path)\n",
    "\n",
    "        parameters, cost = pickle.load(open(save_path, 'rb'))\n",
    "        [filter_one, filter_two, weight_three, weight_four, bias_one, bias_two, bias_three, bias_four] = parameters\n",
    "    \n",
    "        # Plot cost over number of iterations\n",
    "        plt.plot(cost, 'r')\n",
    "        plt.xlabel('Number of Iterations')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.legend('Loss', loc='upper right')\n",
    "        plt.show()\n",
    "\n",
    "        # Get test data\n",
    "        m =10000\n",
    "        util = ModelHelpers()\n",
    "        X = util.extract_dataset('./mnist_data/t10k-images-idx3-ubyte.gz', m, 28)\n",
    "        y_dash = util.extract_desired_labels('./mnist_data/t10k-labels-idx1-ubyte.gz', m).reshape(m,1)\n",
    "        \n",
    "        # Normalize the data\n",
    "        X-= int(np.mean(X)) # subtract mean\n",
    "        X/= int(np.std(X)) # divide by standard deviation\n",
    "        test_data = np.hstack((X,y_dash))\n",
    "    \n",
    "        X = test_data[:,0:-1]\n",
    "        X = X.reshape(len(test_data), 1, 28, 28)\n",
    "        y = test_data[:,-1]\n",
    "\n",
    "        corr = 0\n",
    "        digit_count = [0 for inputs in range(10)]\n",
    "        digit_correct = [0 for inputs in range(10)]\n",
    "   \n",
    "        print()\n",
    "        print(\"Next, Computing accuracy operation on the test dataset:\")\n",
    "\n",
    "        t = tqdm(range(len(X)), leave=True)\n",
    "\n",
    "        for inputs in t:\n",
    "            x = X[inputs]\n",
    "            pred, prob = util.predict(x, filter_one, filter_two, weight_three, weight_four, bias_one, bias_two, bias_three, bias_four)\n",
    "            digit_count[int(y[inputs])]+=1\n",
    "            if pred==y[inputs]:\n",
    "                corr+=1\n",
    "                digit_correct[pred]+=1\n",
    "\n",
    "            t.set_description(\"Acc:%0.2f%%\" % (float(corr/(inputs+1))*100))\n",
    "        \n",
    "        print(\"Overall Accuracy: %.2f\" % (float(corr/len(test_data)*100)))\n",
    "        x = np.arange(10)\n",
    "        digit_recall = [x/y for x,y in zip(digit_correct, digit_count)]\n",
    "        plt.xlabel('Digits')\n",
    "        plt.ylabel('Recall')\n",
    "        plt.title(\"Recall on Test Set\")\n",
    "        plt.bar(x,digit_recall)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run network.py parameters.pkl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
